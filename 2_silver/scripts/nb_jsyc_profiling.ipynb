{"cells":[{"cell_type":"code","source":["table_name = 'games'\n","run_id = None\n","stage = 'lh_silver'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"85777a49-84bd-4518-9c9c-e0cadac89343","normalized_state":"finished","queued_time":"2025-07-27T11:08:47.5638897Z","session_start_time":null,"execution_start_time":"2025-07-27T11:08:47.5650506Z","execution_finish_time":"2025-07-27T11:08:47.8392293Z","parent_msg_id":"c5e8d5b1-2633-44ac-847b-19f3237550cb"},"text/plain":"StatementMeta(, 85777a49-84bd-4518-9c9c-e0cadac89343, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"18d93880-1192-470d-a547-74896c64afbc"},{"cell_type":"code","source":["from typing import Tuple, List\n","from datetime import datetime\n","from pyspark.sql import DataFrame\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","\n","class RawTableProfiler:\n","    \"\"\"\n","    Column-wise profiler that works in Microsoft Fabric (PySpark).\n","    Produces:\n","      - metrics_df: one row per column with completeness, distincts, stats, etc.\n","      - topk_df:    top-k most frequent values per column (optional)\n","    \"\"\"\n","\n","    def __init__(self, spark, run_id: str, table_name: str):\n","        self.spark = spark\n","        self.run_id = run_id\n","        self.table_name = table_name\n","        self.profiled_ts = datetime.utcnow()\n","\n","\n","    @staticmethod\n","    def _is_numeric(dt: T.DataType) -> bool:\n","        return isinstance(dt, (T.ByteType, T.ShortType, T.IntegerType, T.LongType,\n","                               T.FloatType, T.DoubleType, T.DecimalType))\n","\n","    @staticmethod\n","    def _is_string(dt: T.DataType) -> bool:\n","        return isinstance(dt, T.StringType)\n","\n","    @staticmethod\n","    def _is_bool(dt: T.DataType) -> bool:\n","        return isinstance(dt, T.BooleanType)\n","\n","    @staticmethod\n","    def _is_datetime(dt: T.DataType) -> bool:\n","        return isinstance(dt, (T.TimestampType, T.DateType))\n","\n","    @staticmethod\n","    def _safe_alias(expr, name):\n","        return expr.alias(name)\n","\n","    # ---------- main\n","\n","    def profile(self, df: DataFrame, topk: int = 10) -> Tuple[DataFrame, DataFrame]:\n","        \"\"\"\n","        Compute metrics for each column in df.\n","        Returns (metrics_df, topk_df).\n","        \"\"\"\n","        total_rows = df.count()\n","\n","        metrics_rows: List[DataFrame] = []\n","        topk_rows: List[DataFrame] = []\n","\n","        for c in df.columns:\n","            dt = dict(df.dtypes)[c]  # string dtype name\n","            dtype_obj = df.schema[c].dataType\n","\n","            col = F.col(c)\n","\n","            non_null = F.count(col)\n","            nulls = F.lit(total_rows) - non_null\n","            completeness = (non_null / F.lit(total_rows)).cast(\"double\")\n","\n","            approx_distinct = F.approx_count_distinct(col).cast(\"long\")\n","            distinct_ratio = (approx_distinct / F.lit(total_rows)).cast(\"double\")\n","\n","            min_value = F.lit(None).cast(\"string\")\n","            max_value = F.lit(None).cast(\"string\")\n","            mean_value = F.lit(None).cast(\"double\")\n","            stddev_value = F.lit(None).cast(\"double\")\n","            p05 = F.lit(None).cast(\"double\")\n","            p50 = F.lit(None).cast(\"double\")\n","            p95 = F.lit(None).cast(\"double\")\n","            min_len = F.lit(None).cast(\"int\")\n","            avg_len = F.lit(None).cast(\"double\")\n","            max_len = F.lit(None).cast(\"int\")\n","            empty_strings = F.lit(None).cast(\"long\")\n","            true_count = F.lit(None).cast(\"long\")\n","            false_count = F.lit(None).cast(\"long\")\n","\n","            inferred_semantic = \"unknown\"\n","\n","            if self._is_numeric(dtype_obj):\n","                inferred_semantic = \"numeric\"\n","                min_value = F.min(col).cast(\"string\")\n","                max_value = F.max(col).cast(\"string\")\n","                mean_value = F.avg(col).cast(\"double\")\n","                stddev_value = F.stddev_samp(col).cast(\"double\")\n","                pcts = F.percentile_approx(col, [0.05, 0.5, 0.95], 10000)\n","                # percentile_approx returns array<double>\n","                p05 = F.element_at(pcts, 1).cast(\"double\")\n","                p50 = F.element_at(pcts, 2).cast(\"double\")\n","                p95 = F.element_at(pcts, 3).cast(\"double\")\n","\n","            elif self._is_datetime(dtype_obj):\n","                inferred_semantic = \"datetime\"\n","                min_value = F.min(col.cast(\"timestamp\")).cast(\"string\")\n","                max_value = F.max(col.cast(\"timestamp\")).cast(\"string\")\n","                # Percentiles for datetime: convert to long epoch seconds\n","                epoch_col = F.col(c).cast(\"timestamp\").cast(\"long\")\n","                pcts = F.percentile_approx(epoch_col, [0.05, 0.5, 0.95], 10000)\n","                p05 = F.element_at(pcts, 1).cast(\"double\")\n","                p50 = F.element_at(pcts, 2).cast(\"double\")\n","                p95 = F.element_at(pcts, 3).cast(\"double\")\n","\n","            elif self._is_bool(dtype_obj):\n","                inferred_semantic = \"boolean\"\n","                true_count = F.sum(F.when(col.isNotNull() & (col.cast(\"boolean\") == True), 1).otherwise(0)).cast(\"long\")\n","                false_count = F.sum(F.when(col.isNotNull() & (col.cast(\"boolean\") == False), 1).otherwise(0)).cast(\"long\")\n","                min_value = F.min(col.cast(\"string\"))\n","                max_value = F.max(col.cast(\"string\"))\n","\n","            elif self._is_string(dtype_obj):\n","                inferred_semantic = \"string\"\n","                length_col = F.length(col)\n","                min_len = F.min(length_col).cast(\"int\")\n","                avg_len = F.avg(length_col).cast(\"double\")\n","                max_len = F.max(length_col).cast(\"int\")\n","                empty_strings = F.sum(F.when(col == \"\", 1).otherwise(0)).cast(\"long\")\n","                # For strings, keep min/max lexicographically\n","                min_value = F.min(col).cast(\"string\")\n","                max_value = F.max(col).cast(\"string\")\n","\n","            # Build a single-row aggregation for this column\n","            agg_exprs = [\n","                self._safe_alias(F.lit(self.run_id), \"run_id\"),\n","                self._safe_alias(F.lit(self.table_name), \"table_name\"),\n","                self._safe_alias(F.lit(c), \"column_name\"),\n","                self._safe_alias(F.lit(dt), \"data_type\"),\n","                self._safe_alias(F.lit(total_rows).cast(\"long\"), \"total_rows\"),\n","                self._safe_alias(non_null.cast(\"long\"), \"non_null_rows\"),\n","                self._safe_alias(nulls.cast(\"long\"), \"null_rows\"),\n","                self._safe_alias(completeness, \"completeness_ratio\"),\n","                self._safe_alias(approx_distinct, \"distinct_count\"),\n","                self._safe_alias(distinct_ratio, \"distinct_ratio\"),\n","                self._safe_alias(min_value, \"min_value\"),\n","                self._safe_alias(max_value, \"max_value\"),\n","                self._safe_alias(mean_value, \"mean\"),\n","                self._safe_alias(stddev_value, \"stddev\"),\n","                self._safe_alias(p05, \"p05\"),\n","                self._safe_alias(p50, \"p50\"),\n","                self._safe_alias(p95, \"p95\"),\n","                self._safe_alias(min_len, \"min_length\"),\n","                self._safe_alias(avg_len, \"avg_length\"),\n","                self._safe_alias(max_len, \"max_length\"),\n","                self._safe_alias(empty_strings, \"empty_string_count\"),\n","                self._safe_alias(true_count, \"true_count\"),\n","                self._safe_alias(false_count, \"false_count\"),\n","                self._safe_alias(F.lit(inferred_semantic), \"inferred_semantic\"),\n","                self._safe_alias(F.lit(self.profiled_ts), \"profiled_ts\"),\n","            ]\n","\n","            metrics_row = df.agg(*agg_exprs)\n","            metrics_rows.append(metrics_row)\n","\n","            # Top-K frequency for this column (stringify values for consistency)\n","            topk_df = (\n","                df.groupBy(col.cast(\"string\").alias(\"value\"))\n","                  .agg(F.count(F.lit(1)).alias(\"cnt\"))\n","                  .orderBy(F.desc(\"cnt\"))\n","                  .limit(topk)\n","                  .withColumn(\"run_id\", F.lit(self.run_id))\n","                  .withColumn(\"table_name\", F.lit(self.table_name))\n","                  .withColumn(\"column_name\", F.lit(c))\n","                  .withColumn(\"profiled_ts\", F.lit(self.profiled_ts))\n","                  .select(\"run_id\", \"table_name\", \"column_name\", \"value\", \"cnt\", \"profiled_ts\")\n","            )\n","            topk_rows.append(topk_df)\n","\n","        metrics_df = metrics_rows[0]\n","        for r in metrics_rows[1:]:\n","            metrics_df = metrics_df.unionByName(r)\n","\n","        topk_df_all = topk_rows[0]\n","        for r in topk_rows[1:]:\n","            topk_df_all = topk_df_all.unionByName(r)\n","\n","        return metrics_df, topk_df_all\n","\n","\n","    def ensure_tables(self,\n","                      metrics_table: str = \"monitoring_profile_metrics\",\n","                      topk_table: str = \"monitoring_profile_topk\") -> None:\n","        self.spark.sql(f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {metrics_table} (\n","            run_id STRING,\n","            table_name STRING,\n","            column_name STRING,\n","            data_type STRING,\n","            total_rows BIGINT,\n","            non_null_rows BIGINT,\n","            null_rows BIGINT,\n","            completeness_ratio DOUBLE,\n","            distinct_count BIGINT,\n","            distinct_ratio DOUBLE,\n","            min_value STRING,\n","            max_value STRING,\n","            mean DOUBLE,\n","            stddev DOUBLE,\n","            p05 DOUBLE,\n","            p50 DOUBLE,\n","            p95 DOUBLE,\n","            min_length INT,\n","            avg_length DOUBLE,\n","            max_length INT,\n","            empty_string_count BIGINT,\n","            true_count BIGINT,\n","            false_count BIGINT,\n","            inferred_semantic STRING,\n","            profiled_ts TIMESTAMP\n","        ) USING DELTA\n","        \"\"\")\n","\n","        self.spark.sql(f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {topk_table} (\n","            run_id STRING,\n","            table_name STRING,\n","            column_name STRING,\n","            value STRING,\n","            cnt BIGINT,\n","            profiled_ts TIMESTAMP\n","        ) USING DELTA\n","        \"\"\")\n","\n","    def write(self,\n","              metrics_df: DataFrame,\n","              topk_df: DataFrame,\n","              metrics_table: str = \"monitoring_profile_metrics\",\n","              topk_table: str = \"monitoring_profile_topk\") -> None:\n","        metrics_df.write.mode(\"append\").saveAsTable(metrics_table)\n","        topk_df.write.mode(\"append\").saveAsTable(topk_table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"85777a49-84bd-4518-9c9c-e0cadac89343","normalized_state":"finished","queued_time":"2025-07-27T11:08:47.6822138Z","session_start_time":null,"execution_start_time":"2025-07-27T11:08:47.84172Z","execution_finish_time":"2025-07-27T11:08:48.123537Z","parent_msg_id":"de721a9a-046c-45ce-a251-897910985e9c"},"text/plain":"StatementMeta(, 85777a49-84bd-4518-9c9c-e0cadac89343, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"88589f14-65f3-4276-b506-17f45bbbf4fc"},{"cell_type":"code","source":["df = spark.sql(f\"SELECT * FROM {stage}.{table_name}\")\n","\n","profiler = RawTableProfiler(\n","    spark=spark,\n","    run_id=run_id,  \n","    table_name=f\"{stage}.{table_name}\"\n",")\n","\n","metrics_df, topk_df = profiler.profile(df, topk=10)\n","\n","profiler.ensure_tables()\n","profiler.write(metrics_df, topk_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"85777a49-84bd-4518-9c9c-e0cadac89343","normalized_state":"finished","queued_time":"2025-07-27T11:08:47.7371354Z","session_start_time":null,"execution_start_time":"2025-07-27T11:08:48.1259917Z","execution_finish_time":"2025-07-27T11:09:06.452643Z","parent_msg_id":"409764f7-a0f0-4911-818a-4eb0d7db5549"},"text/plain":"StatementMeta(, 85777a49-84bd-4518-9c9c-e0cadac89343, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0306264-4348-48e8-800b-1376ecffc216"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"ffb706a9-1403-4b71-92b4-5f730521f14a"},{"id":"57f03984-7250-4857-ae82-16265cc7cd2d"},{"id":"5695a2bd-534e-4dec-88ea-69b96182e8dd"}],"default_lakehouse":"57f03984-7250-4857-ae82-16265cc7cd2d","default_lakehouse_name":"main","default_lakehouse_workspace_id":"29b4ee29-00d3-4a8c-833d-5e5f070916e1"},"environment":{"environmentId":"c11023e4-9e95-497b-ba16-a677e06efc1f","workspaceId":"29b4ee29-00d3-4a8c-833d-5e5f070916e1"}}},"nbformat":4,"nbformat_minor":5}