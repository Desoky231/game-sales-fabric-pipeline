{"cells":[{"cell_type":"code","source":["table_name = 'promotions'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"b27a3f6d-e08b-438c-8dcf-579032102320","normalized_state":"finished","queued_time":"2025-07-27T09:27:58.746699Z","session_start_time":"2025-07-27T09:27:58.7477148Z","execution_start_time":"2025-07-27T09:28:11.1249645Z","execution_finish_time":"2025-07-27T09:28:11.481901Z","parent_msg_id":"9d9c833a-f66d-4890-8681-b670caa96ced"},"text/plain":"StatementMeta(, b27a3f6d-e08b-438c-8dcf-579032102320, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"ba2ea182-c9ff-4ffc-bc4d-eaffbf1d8598"},{"cell_type":"code","source":["from pathlib import Path\n","import json, logging\n","from typing import Optional, Dict, Any\n","from pyspark.sql import SparkSession, DataFrame, Window\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col,to_date, lower, trim, regexp_replace, explode, split, current_date, lit, udf,round as spark_round\n","import re\n","from datetime import datetime"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"b27a3f6d-e08b-438c-8dcf-579032102320","normalized_state":"finished","queued_time":"2025-07-27T09:27:58.9166627Z","session_start_time":null,"execution_start_time":"2025-07-27T09:28:11.4838908Z","execution_finish_time":"2025-07-27T09:28:11.7684643Z","parent_msg_id":"0bba22d2-35e5-4e95-a4c5-a5cdf779a584"},"text/plain":"StatementMeta(, b27a3f6d-e08b-438c-8dcf-579032102320, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79369c70-4c82-4a8c-86a7-75bb9d94c3b6"},{"cell_type":"code","source":["\n","def parse_int_safe(val):\n","    try:\n","        return int(float(val)) if val is not None else None\n","    except:\n","        return None\n","\n","def parse_float_safe(val):\n","    try:\n","        return float(val) if val is not None else None\n","    except:\n","        return None\n","\n","int_udf = udf(parse_int_safe, LongType())\n","float_udf = udf(parse_float_safe, DoubleType())\n","\n","\n","class DataCleaningSpark:\n","\n","    def __init__(self, spark: SparkSession, source_db: str, table_name: str, meta_data_table: str, target_db: str = \"lh_transformation\"):\n","        self.spark = spark\n","        self.table_name = table_name\n","        self.target_db = target_db\n","        self.df = spark.sql(f\"SELECT * FROM {source_db}.{table_name}\")\n","        meta_raw = spark.sql(f\"SELECT * FROM main.{meta_data_table}\")\n","        self.meta = (\n","            meta_raw.filter(F.col(\"source_table\") == table_name)\n","                    .withColumn(\"column\", F.lower(F.regexp_replace(F.col(\"column\"), r\"\\s+\", \"_\")))\n","        )\n","        self.normalized_tables = {}\n","\n","        if self.meta.count() == 0:\n","            raise ValueError(f\"No metadata for table '{table_name}' in {meta_data_table}\")\n","\n","    def _get_cols(self, flag: str, value: int = 1):\n","        return (\n","            self.meta.filter(F.col(flag) == value)\n","                     .select(\"column\")\n","                     .rdd.flatMap(lambda r: r)\n","                     .collect()\n","        )\n","\n","\n","\n","    def _clean_promotions(self):\n","        \n","        promos = self.df\n","        a = promos.alias(\"a\")\n","        b = promos.alias(\"b\")\n","\n","        pairs = (\n","            a.join(\n","                b,\n","                (F.col(\"a.game_id\") == F.col(\"b.game_id\")) &\n","                (F.col(\"a.promotion_id\") != F.col(\"b.promotion_id\")) &\n","                (F.col(\"a.start_date\") <= F.col(\"b.end_date\")) &      # date ranges intersect\n","                (F.col(\"a.end_date\")   >= F.col(\"b.start_date\")) &\n","                (F.col(\"a.start_date\") <  F.col(\"b.start_date\"))      # a is the older promo\n","            )\n","        )\n","\n","        trim_later = pairs.where(F.col(\"a.percentage\") >= F.col(\"b.percentage\")) \\\n","            .select(\n","                F.col(\"b.promotion_id\").alias(\"promotion_id\"),\n","                (F.col(\"a.end_date\") + F.expr(\"INTERVAL 1 DAY\")).alias(\"new_start\")\n","            )\n","\n","        trim_older = pairs.where(F.col(\"a.percentage\") < F.col(\"b.percentage\")) \\\n","            .select(\n","                F.col(\"a.promotion_id\").alias(\"promotion_id\"),\n","                (F.col(\"b.start_date\") - F.expr(\"INTERVAL 1 DAY\")).alias(\"new_end\")\n","            )\n","\n","        new_starts = (trim_later\n","                    .groupBy(\"promotion_id\")\n","                    .agg(F.max(\"new_start\").alias(\"new_start\")))     # later promo moves *forward*\n","\n","        new_ends = (trim_older\n","                    .groupBy(\"promotion_id\")\n","                    .agg(F.min(\"new_end\").alias(\"new_end\")))         # older promo moves *backward*\n","\n","        trimmed = (\n","            promos\n","            .join(new_starts, on=\"promotion_id\", how=\"left\")\n","            .join(new_ends,   on=\"promotion_id\", how=\"left\")\n","            .withColumn(\n","                \"start_date\",\n","                F.coalesce(\"new_start\", \"start_date\")\n","            )\n","            .withColumn(\n","                \"end_date\",\n","                F.coalesce(\"new_end\", \"end_date\")\n","            )\n","            .drop(\"new_start\", \"new_end\")\n","            .filter(\"start_date <= end_date\")  # discard zero/negative length promos\n","        )\n","\n","        dedupe_win = (\n","            Window\n","            .partitionBy(\"game_id\", \"start_date\", \"end_date\")      # exact same interval\n","            .orderBy(F.col(\"percentage\").desc(), F.col(\"promotion_id\").asc())\n","        )\n","\n","        trimmed = (\n","            trimmed\n","            .withColumn(\"rn\", F.row_number().over(dedupe_win))     # rank within identical windows\n","            .filter(\"rn = 1\")                                      # keep the winner\n","            .drop(\"rn\")\n","        )\n","\n","        # Handle duplicate start dates by pushing lower discount start date after higher discount's end date\n","        start_date_win = (\n","            Window\n","            .partitionBy(\"game_id\", \"start_date\")\n","            .orderBy(F.col(\"percentage\").desc(), F.col(\"promotion_id\").asc())\n","        )\n","\n","        # Identify promotions to adjust\n","        adjust_starts = (\n","            trimmed\n","            .withColumn(\"rn\", F.row_number().over(start_date_win))\n","            .withColumn(\n","                \"max_discount_end\",\n","                F.max(\n","                    F.when(F.col(\"rn\") == 1, F.col(\"end_date\")).otherwise(F.lit(None))\n","                ).over(Window.partitionBy(\"game_id\", \"start_date\"))\n","            )\n","            .filter(F.col(\"rn\") > 1)  # Lower discount promos\n","            .select(\n","                F.col(\"promotion_id\"),\n","                (F.col(\"max_discount_end\") + F.expr(\"INTERVAL 1 DAY\")).alias(\"new_start\")\n","            )\n","        )\n","\n","        # Apply start date adjustments\n","        trimmed = (\n","            trimmed\n","            .join(adjust_starts, on=\"promotion_id\", how=\"left\")\n","            .withColumn(\n","                \"start_date\",\n","                F.coalesce(\"new_start\", \"start_date\")\n","            )\n","            .drop(\"new_start\")\n","            .filter(\"start_date <= end_date\")  # Remove zero/negative length promos\n","        )\n","\n","        # Reapply deduplication to handle any new overlaps caused by adjustments\n","        trimmed = (\n","            trimmed\n","            .withColumn(\"rn\", F.row_number().over(dedupe_win))\n","            .filter(\"rn = 1\")\n","            .drop(\"rn\")\n","        )\n","\n","        self.df = trimmed\n","        return self\n","\n","    def validate_primary_keys(self):\n","        pk_cols = self._get_cols(\"PK\")\n","        if pk_cols:\n","            self.df = self.df.filter(F.expr(\" AND \".join([f\"{c} IS NOT NULL\" for c in pk_cols])))\n","            self.df = self.df.dropDuplicates(pk_cols)\n","        return self\n","\n","    def validate_non_nulls(self):\n","        nn_cols = self._get_cols(\"NON_NULLABLE\")\n","        if nn_cols:\n","            self.df = self.df.filter(F.expr(\" AND \".join([f\"{c} IS NOT NULL\" for c in nn_cols])))\n","        return self\n","\n","    def validate_unique(self):\n","        uniq_cols = self._get_cols(\"UNIQUE\")\n","        if not uniq_cols:\n","            return self\n","        dedup = (\n","            self.df.groupBy(uniq_cols)\n","                   .count()\n","                   .filter(\"count = 1\")\n","                   .drop(\"count\")\n","        )\n","        self.df = self.df.join(dedup, on=uniq_cols, how=\"inner\")\n","        return self\n","\n","    def _parse_dtype(self, dtype_str):\n","        dtype_str = dtype_str.lower()\n","        if dtype_str.startswith(\"int\"):\n","            return LongType()\n","        elif dtype_str.startswith(\"float\") or dtype_str.startswith(\"double\"):\n","            return DoubleType()\n","        elif dtype_str.startswith(\"date\") or dtype_str.startswith(\"datetime\"):\n","            return DateType()\n","        else:\n","            return StringType()\n","\n","    def validate_datatype(self):\n","        fields = []\n","        date_columns = {}\n","        float_precision = {}\n","        integer_columns = []\n","\n","        for row in self.meta.collect():\n","            col_name = row[\"column\"]\n","            dtype = str(row[\"DTYPE\"])\n","\n","            if col_name not in self.df.columns:\n","                continue\n","\n","            base_type = dtype.split(\"|\")[0].lower()\n","\n","            if \"date\" in base_type:\n","                date_columns[col_name] = dtype.split(\"|\")[1]\n","            elif \"float\" in base_type or \"double\" in base_type:\n","                float_precision[col_name] = int(dtype.split(\"|\")[1]) if \"|\" in dtype else 2\n","            elif \"int\" in base_type:\n","                integer_columns.append(col_name)\n","\n","            fields.append(StructField(col_name, self._parse_dtype(dtype), True))\n","\n","        for field in fields:\n","            name = field.name\n","            dtype = field.dataType\n","\n","            if isinstance(dtype, DateType) and name in date_columns:\n","                self.df = self.df.withColumn(name, to_date(col(name), date_columns[name]))\n","            elif isinstance(dtype, DoubleType) and name in float_precision:\n","                self.df = self.df.withColumn(name, spark_round(float_udf(col(name)), float_precision[name]))\n","            elif isinstance(dtype, LongType) and name in integer_columns:\n","                self.df = self.df.withColumn(name, int_udf(col(name)))\n","            else:\n","                self.df = self.df.withColumn(name, col(name).cast(dtype))\n","\n","        return self\n","\n","    def _clean_developers_column(self, col_name: str):\n","        self.df = (\n","            self.df\n","            .withColumn(col_name, F.regexp_replace(col_name, r\",?\\s*Inc\\.?\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\"\\s*\\(Mac\\)\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\"\\s*\\(Linux\\)\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\",?\\s*LLC\\.?\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\",?\\s*Ltd\\.?\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\",?\\s*LTD\\.?\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\",?\\s*INC\\.?\", \"\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\"BANDAI NAMCO\", \"Bandai Namco\"))\n","            .withColumn(col_name, F.regexp_replace(col_name, r\"CAPCOM CO.\", \"CAPCOM Co.\"))\n","            .withColumn(col_name, F.trim(col(col_name)))\n","        )\n","\n","        \n","        return self\n","\n","\n","    def _clean_publishers_column(self, col_name: str):\n","        return self._clean_developers_column(col_name)\n","\n","    def apply_rules(self):\n","        for row in self.meta.collect():\n","            col_name = row[\"column\"]\n","            rules = row[\"RULES\"]\n","\n","            if not rules or col_name not in self.df.columns:\n","                continue\n","\n","            for rule in rules.split(\",\"):\n","                rule = rule.strip().lower()\n","\n","                if rule.startswith(\"range|\"):\n","                    try:\n","                        _, min_val, max_val = rule.split(\"|\")\n","                        max_val = 999999999999 if max_val == 'inf' else max_val\n","                        min_val = -999999999999 if max_val == '-inf' else min_val\n","                        self.df = self.df.filter((col(col_name) >= float(min_val)) & (col(col_name) <= float(max_val)))\n","                    except:\n","                        print(f\"❌ Invalid range rule format: {rule}\")\n","\n","                elif rule == \"developer_clean\":\n","                    self._clean_developers_column(col_name)\n","\n","                elif rule == \"publisher_clean\":\n","                    self._clean_publishers_column(col_name)\n","\n","                elif rule == \"email\":\n","                    self.df = self.df.filter(col(col_name).rlike(r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$\"))\n","\n","                elif rule == \"date_in_past\":\n","                    self.df = self.df.filter(col(col_name) < current_date())\n","\n","                elif rule == \"date_in_past_or_today\":\n","                    self.df = self.df.filter(col(col_name) <= current_date())\n","\n","                elif rule.startswith(\"date_after[\") and rule.endswith(\"]\"):\n","                    ref_col = rule[len(\"date_after[\"):-1]\n","                    if ref_col in self.df.columns:\n","                        self.df = self.df.filter(col(col_name) > col(ref_col))\n","                    else:\n","                        print(f\"❌ Reference column '{ref_col}' not found for rule '{rule}'.\")\n","\n","                elif rule == \"normalize_alpha\":\n","                    self.df = (\n","                        self.df\n","                        .withColumn(\n","                            col_name,\n","                            F.when(\n","                                F.col(col_name).cast(\"string\").rlike(\"[0-9]\") |  # Contains digits\n","                                F.col(col_name).cast(\"string\").rlike(\"^[ ]*$\"),  # Contains only spaces\n","                                F.lit(None)  # Replace with null\n","                            ).otherwise(\n","                                F.lower(F.regexp_replace(F.col(col_name), \"[^a-zA-Z ]\", \"\"))  # Keep letters and spaces, lowercase\n","                            )\n","                        )\n","                        .filter(\n","                            F.col(col_name).isNotNull() &  # Remove nulls\n","                            (F.col(col_name) != \"\")  # Remove empty strings\n","                        )\n","                    )\n","\n","                elif rule == \"promotions_clean\":\n","                    self._clean_promotions()  # Call the new promotions cleaning logic\n","\n","                else:\n","                    print(f\"⚠️ Unknown rule '{rule}' ignored.\")\n","\n","        return self\n","\n","\n","    def normalize_2nf_df(self):\n","            norm_cols = self._get_cols(\"NORMALIZE2NF\")\n","            key_cols = self._get_cols(\"PK\")  \n","\n","            for col in norm_cols:\n","                table_name = f\"{self.table_name}_{col}\"\n","\n","                unique_vals = (\n","                    self.df.select(col)\n","                        .distinct()\n","                        .rdd.flatMap(lambda x: x)\n","                        .collect()\n","                )\n","\n","                ids = list(range(len(unique_vals)))\n","                lookup_data = list(zip(ids, unique_vals))\n","                lookup_df = self.spark.createDataFrame(lookup_data, schema=[\"id\", col])\n","                self.normalized_tables[table_name] = lookup_df\n","\n","                self.df = (\n","                    self.df.join(lookup_df, on=col, how=\"left\")\n","                        .drop(col)\n","                        .withColumnRenamed(\"id\", f\"{col}_id\")\n","                )\n","\n","            return self\n","    def normalize_3nf_df(self):\n","        norm_cols = self._get_cols(\"NORMALIZE3NF\")\n","        key_cols = self._get_cols(\"PK\")\n","\n","        for col in norm_cols:\n","            table_name = f\"{self.table_name}_{col}\"\n","\n","            exploded_df = (\n","                self.df.select(*key_cols, col)\n","                    .withColumn(col, explode(split(col, \",\")))\n","                    .withColumn(col, trim(col))\n","            )\n","\n","            unique_vals = (\n","                exploded_df.select(col)\n","                           .distinct()\n","                           .rdd.flatMap(lambda x: x)\n","                           .collect()\n","            )\n","\n","            ids = list(range(len(unique_vals)))\n","            lookup_data = list(zip(ids, unique_vals))\n","            lookup_df = self.spark.createDataFrame(lookup_data, schema=[\"id\", col])\n","            self.normalized_tables[table_name] = lookup_df\n","\n","            bridge_df = (\n","                exploded_df.join(lookup_df, on=col, how=\"left\")\n","                           .select(*key_cols, \"id\")\n","                           .withColumnRenamed(\"id\", f\"{col}_id\")\n","            )\n","            self.normalized_tables[f\"{self.table_name}_{col}_bridge\"] = bridge_df\n","\n","            self.df = self.df.drop(col)\n","\n","        return self\n","\n","    def save(self, mode: str = \"overwrite\"):\n","        self.df.write.mode(mode).option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(self.table_name)\n","        print(f\"✔️  {self.target_db}.{self.table_name} written ({mode})\")\n","\n","        for table_name, df in self.normalized_tables.items():\n","            df.write.mode(mode).option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(table_name)\n","            print(f\"✔️  {self.target_db}.{table_name} written ({mode})\")\n","\n","        return f\"{self.target_db}.{self.table_name}\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"b27a3f6d-e08b-438c-8dcf-579032102320","normalized_state":"finished","queued_time":"2025-07-27T09:27:59.024871Z","session_start_time":null,"execution_start_time":"2025-07-27T09:28:11.7713906Z","execution_finish_time":"2025-07-27T09:28:12.0598967Z","parent_msg_id":"c1b0f08a-48ff-426a-9b8f-ae39ea26c961"},"text/plain":"StatementMeta(, b27a3f6d-e08b-438c-8dcf-579032102320, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69641c8c-8ec5-4e9e-8b99-bc5eaca0331b"},{"cell_type":"code","source":["status = 'Succeeded'\n","try:\n","    cleaner = DataCleaningSpark(\n","        spark,\n","        source_db=\"lh_bronze\",          \n","        table_name=table_name,\n","        meta_data_table=\"meta_data_silver\",\n","        target_db= \"lh_transformation\"    \n","    )\n","    rows_in = cleaner.df.count()\n","    cleaner = cleaner.validate_datatype()\n","    print(f\"After validate_datatype: {cleaner.df.count()} rows\")\n","\n","    cleaner = cleaner.apply_rules()\n","    print(f\"After applying rules: {cleaner.df.count()} rows\")\n","\n","    cleaner = cleaner.validate_primary_keys()\n","    print(f\"After validate_primary_keys: {cleaner.df.count()} rows\")\n","\n","    cleaner = cleaner.validate_non_nulls()\n","    print(f\"After validate_non_nulls: {cleaner.df.count()} rows\")\n","\n","    cleaner = cleaner.validate_unique()\n","    print(f\"After validate_unique: {cleaner.df.count()} rows\")\n","\n","\n","\n","    cleaner = cleaner.normalize_2nf_df()\n","\n","    cleaner = cleaner.normalize_3nf_df()\n","\n","    rows_out = cleaner.df.count()\n","    cleaner.save(mode=\"overwrite\") \n","\n","except:\n","    status = \"Failed\"\n","\n","finally:\n","    output = {\n","    \"rows_in\": rows_in,\n","    \"rows_out\": rows_out,\n","    \"status\": status,\n","    \"run_ts\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    }\n","\n","    notebookutils.notebook.exit(json.dumps(output))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"b27a3f6d-e08b-438c-8dcf-579032102320","normalized_state":"finished","queued_time":"2025-07-27T09:27:59.2641291Z","session_start_time":null,"execution_start_time":"2025-07-27T09:28:12.0618275Z","execution_finish_time":"2025-07-27T09:29:55.2753507Z","parent_msg_id":"91332be2-eb08-4e4d-86bf-4d10e0a311e5"},"text/plain":"StatementMeta(, b27a3f6d-e08b-438c-8dcf-579032102320, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["After validate_datatype: 190389 rows\n"]},{"output_type":"stream","name":"stdout","text":["After applying rules: 161941 rows\n"]},{"output_type":"stream","name":"stdout","text":["After validate_primary_keys: 161941 rows\n"]},{"output_type":"stream","name":"stdout","text":["After validate_non_nulls: 161941 rows\n"]},{"output_type":"stream","name":"stdout","text":["After validate_unique: 161941 rows\n"]},{"output_type":"stream","name":"stdout","text":["✔️  lh_transformation.promotions written (overwrite)\nExitValue: {\"rows_in\": 190389, \"rows_out\": 161941, \"status\": \"Succeeded\", \"run_ts\": \"2025-07-27 09:29:53\"}"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a0f82cfe-80a0-4920-90ce-111006b6d862"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"ffb706a9-1403-4b71-92b4-5f730521f14a"},{"id":"5695a2bd-534e-4dec-88ea-69b96182e8dd"},{"id":"57f03984-7250-4857-ae82-16265cc7cd2d"}],"default_lakehouse":"5695a2bd-534e-4dec-88ea-69b96182e8dd","default_lakehouse_name":"lh_silver","default_lakehouse_workspace_id":"29b4ee29-00d3-4a8c-833d-5e5f070916e1"}}},"nbformat":4,"nbformat_minor":5}